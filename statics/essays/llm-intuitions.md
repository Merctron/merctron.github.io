# Notes About Intelligence

This is an attempt to write down what I've learned about developments in Artificial Intelligence (AI), and what Intelligence is generally. As I begin writing this, transformer based language models (the use of which has dominated my day job more recently) dominate headlines so I will use them as a starting point.

## Compute VS Self-Modeling

I'm paraphrasing here from [the bitter lesson by Richard Sutton](http://www.incompleteideas.net/IncIdeas/BitterLesson.html) - Every time humans have tried to model their own intelligence to build AI, they have run into limits. We tried to build logic driven inference systems to distill the full understanding of domains into thinking machines and that hit a wall after a hype cycle leading and ensuing AI winter. What seems to work, is simple mechanisms that can build upon themselves to emerge with something complex - basics like search, and reinforcement. These simple mechanisms have to be paired with advances in compute to get the truly great results.


So far (02/2025), this thesis is holding true - large language models (LLMs) are performing very well with a simple idea (attention/self-attention) that has combined with an explosion of compute. I have not made up my mind about whether this paradigm will continue to pay dividends or whether some extra symbolic component will be needed to build something even more general and resilient to approach the abilities of a human brain - but the possibility cannot be ruled out.

## The Simple Mechanisms



